Feature: Backup and Vacuum Operations for customer_360_raw Table

  The system must provide a PySpark script to back up the purgo_databricks.purgo_playground.customer_360_raw table as compressed parquet files partitioned by state in a Databricks volume, and perform a vacuum operation to retain only records from the last 30 days.

  Background:
    Given the Unity Catalog is set to "purgo_databricks"
    And the schema is set to "purgo_playground"
    And the source table is "customer_360_raw"
    And the backup volume path is "/Volumes/customer_360_raw_backup"
    And the backup must be written as parquet files with "snappy" compression
    And the parquet files must be partitioned by the "state" column
    And the backup must include all columns from the source table without transformation
    And the backup must be a full snapshot, overwriting any previous backup at the target location
    And the expected folder structure is "/Volumes/customer_360_raw_backup/state=XX/"
    And the table "customer_360_raw" is a Delta table
    And the date column for retention is "creation_date"
    And the vacuum operation must retain only records with "creation_date" within the last 30 days (inclusive)
    And the vacuum operation must use the default retention period of 168 hours (7 days)
    And all operations must be logged to "customer_360_raw_backup_log" with timestamp, status, operation_type, record_count, and error_message

  Scenario: Successful backup of customer_360_raw table to compressed parquet files partitioned by state
    Given the "customer_360_raw" table contains records with valid data in all columns
    When the backup PySpark script is executed
    Then parquet files are written to "/Volumes/customer_360_raw_backup" partitioned by "state"
    And the parquet files use "snappy" compression
    And all columns from the source table are present in the backup files
    And the backup operation overwrites any existing data at the backup location
    And the record count in the backup matches the record count in the source table
    And a log entry is created in "customer_360_raw_backup_log" with status "SUCCESS", operation_type "BACKUP", the correct record_count, and error_message as null

  Scenario: Backup fails due to missing backup volume path
    Given the backup volume path "/Volumes/customer_360_raw_backup" does not exist
    When the backup PySpark script is executed
    Then the backup operation fails
    And no parquet files are written
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Backup volume path not found"

  Scenario: Backup fails due to missing "state" column in source table
    Given the "customer_360_raw" table does not contain the "state" column
    When the backup PySpark script is executed
    Then the backup operation fails
    And no parquet files are written
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Partition column 'state' not found"

  Scenario: Successful deletion of records older than 30 days and vacuum operation
    Given the "customer_360_raw" table contains records with "creation_date" older and newer than 30 days from current date
    When the vacuum PySpark script is executed
    Then all records with "creation_date" older than 30 days are deleted from the table
    And only records with "creation_date" within the last 30 days (inclusive) remain
    And the Delta VACUUM command is executed with the default retention period of 168 hours
    And obsolete files are removed from storage
    And a log entry is created in "customer_360_raw_backup_log" with status "SUCCESS", operation_type "VACUUM", the correct record_count (remaining records), and error_message as null

  Scenario: Vacuum operation fails due to non-Delta table
    Given the "customer_360_raw" table is not a Delta table
    When the vacuum PySpark script is executed
    Then the vacuum operation fails
    And no records are deleted
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "VACUUM", record_count as 0, and error_message containing "VACUUM operation requires a Delta table"

  Scenario: Vacuum operation fails due to missing "creation_date" column
    Given the "customer_360_raw" table does not contain the "creation_date" column
    When the vacuum PySpark script is executed
    Then the vacuum operation fails
    And no records are deleted
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "VACUUM", record_count as 0, and error_message containing "Date column 'creation_date' not found"

  Scenario Outline: Data validation for backup and vacuum operations
    Given the "customer_360_raw" table contains <record_count> records
    And <invalid_condition>
    When the <operation> PySpark script is executed
    Then the operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "<operation_type>", record_count as 0, and error_message containing "<expected_error>"

    Examples:
      | record_count | invalid_condition                                 | operation | operation_type | expected_error                                 |
      | 1000         | some records have null in "state" column          | backup    | BACKUP        | Null values found in partition column 'state'   |
      | 500          | "creation_date" column contains invalid date      | vacuum    | VACUUM        | Invalid date format in 'creation_date' column   |
      | 0            | table is empty                                    | backup    | BACKUP        | Source table is empty                          |
      | 0            | table is empty                                    | vacuum    | VACUUM        | Source table is empty                          |

  Scenario: Validation of backup completeness and data integrity
    Given the backup operation has completed successfully
    When the record count in the backup files is compared to the source table
    Then the counts must match exactly
    And the schema of the backup files must match the source table schema
    And all data values must be identical between source and backup

  Scenario: Logging of backup and vacuum operations
    Given a backup or vacuum operation is performed
    When the operation completes (success or failure)
    Then a log entry is created in "customer_360_raw_backup_log" with the following fields:
      | timestamp         | string, ISO 8601 format, not null           |
      | status            | string, values: "SUCCESS" or "FAILURE"      |
      | operation_type    | string, values: "BACKUP" or "VACUUM"        |
      | record_count      | bigint, number of records processed         |
      | error_message     | string, nullable, error details if failure  |

  Scenario: Backup operation with incremental logic (if specified)
    Given the backup mode is set to "incremental"
    And the last backup timestamp is "2024-07-01T00:00:00Z"
    When the backup PySpark script is executed
    Then only records in "customer_360_raw" with "creation_date" > "2024-07-01" are written to the backup location
    And the backup files are partitioned by "state"
    And a log entry is created in "customer_360_raw_backup_log" with status "SUCCESS", operation_type "BACKUP", the correct record_count, and error_message as null

  Scenario: Backup operation with timestamped/versioned output (if specified)
    Given the backup mode is set to "versioned"
    And the current timestamp is "2024-07-30T12:00:00Z"
    When the backup PySpark script is executed
    Then the backup files are written to "/Volumes/customer_360_raw_backup/2024-07-30T12-00-00/"
    And the files are partitioned by "state"
    And a log entry is created in "customer_360_raw_backup_log" with status "SUCCESS", operation_type "BACKUP", the correct record_count, and error_message as null

  Scenario: Rollback strategy before vacuum (if required)
    Given a backup operation has completed successfully
    And a vacuum operation is about to be performed
    When the vacuum PySpark script is executed
    Then the backup files must be available for restore in case of failure
    And a log entry is created in "customer_360_raw_backup_log" with status "SUCCESS", operation_type "VACUUM", the correct record_count, and error_message as null

  Scenario: Error handling for insufficient permissions
    Given the user executing the script does not have write permissions to the backup volume
    When the backup PySpark script is executed
    Then the backup operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Permission denied"

  Scenario: Error handling for insufficient permissions on source table
    Given the user executing the script does not have read permissions on "customer_360_raw"
    When the backup or vacuum PySpark script is executed
    Then the operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "<operation_type>", record_count as 0, and error_message containing "Permission denied"

  Scenario: Error handling for invalid compression type
    Given the backup compression type is set to "invalid_codec"
    When the backup PySpark script is executed
    Then the backup operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Unsupported compression codec"

  Scenario: Error handling for invalid backup location path format
    Given the backup volume path is set to an invalid format (e.g., "C:\invalid\path")
    When the backup PySpark script is executed
    Then the backup operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Invalid backup path format"

  Scenario: Error handling for missing required columns in backup
    Given the "customer_360_raw" table is missing one or more required columns
    When the backup PySpark script is executed
    Then the backup operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Missing required columns"

  Scenario: Error handling for data type mismatch in backup
    Given the "customer_360_raw" table contains a column with an unexpected data type
    When the backup PySpark script is executed
    Then the backup operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Data type mismatch"

  Scenario: Error handling for backup operation timeout
    Given the backup operation exceeds the maximum allowed execution time of 2 hours
    When the backup PySpark script is executed
    Then the backup operation fails
    And a log entry is created in "customer_360_raw_backup_log" with status "FAILURE", operation_type "BACKUP", record_count as 0, and error_message containing "Backup operation timed out"
